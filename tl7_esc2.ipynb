{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python_defaultSpec_1599433427500",
      "display_name": "Python 3.8.5 64-bit"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lFfgGb6pyjkS",
        "mU7FF2cY_k6h",
        "0gIjmRUdupCE",
        "oWPr05d153o7"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk3R2fku35ux"
      },
      "source": [
        "# Abrir fuente de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo9YXMZ2McfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fd8a48-11c2-46ee-a8c3-8ccf55e0dfb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGmMgV72DU8Q"
      },
      "source": [
        "#Elimina folder\n",
        "#!rm -rf LC08\n",
        "!rm -rf spectrograms224_00\n",
        "#!rm -rf spectrograms224_00/TC\n",
        "#!rm -rf sismograms224_00\n",
        "#!rm -rf sismograms224_00/TC"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2UkoGMfZQrG"
      },
      "source": [
        "# Descomprimirlo\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Sismogramas/Escenario2/00/sismograms224_00.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +da_rotacion_25_45.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion/spectrograms224_00-01I.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708.zip' &> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VizFcfusBmay"
      },
      "source": [
        "# Instalando librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4pX_ZkxBk5D"
      },
      "source": [
        "#TorchMetrics\n",
        "!pip install torchmetrics &> /dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPOSdxW9CDqz"
      },
      "source": [
        "## Parámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSFINwxfCAOF"
      },
      "source": [
        "# Define parámetros\n",
        "class TL:   # Modelos pre-entrenados\n",
        "  AlexNet   =\"AlexNet\"\n",
        "  GoogLeNet =\"GoogLeNet\"\n",
        "  ResNet    =\"ResNet152\"\n",
        "  VGG       =\"VGG19bn\"\n",
        "class FLAGS:\n",
        "  tls = [TL.AlexNet]          # Modelo TL a considerar en proceso\n",
        "  carpeta       = 'spectrograms224_00'      # Carpeta de espectrogramas\n",
        "  classes       = ['HY','LP','TC','TR','VT']  # Lista de clases\n",
        "  num_classes   = 5           # Número de clases\n",
        "  batch_size    = 8           # Genera el dataset torch de los espectrogramas\n",
        "  num_workers   = 2           # Procesos\n",
        "  learning_rate = 5e-5        # Tasa de aprendizaje\n",
        "  num_epochs    = 20          # Número de épocas de entrenamiento\n",
        "  iPorcentajeTiempoInicio = 5           # Porcentaje de tiempo de señal para rotación inicio. Rango 0-100.\n",
        "  iPorcentajeTiempoFinal  = 6           # Porcentaje de tiempo de señal para rotación fin. Rango 0-100.\n",
        "  iPorcentajeTiempoInc    = 5           # Incremento\n",
        "  iPorcentajeTiempoLong   = 20          # Longitud de rangos porcentuales."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFfgGb6pyjkS"
      },
      "source": [
        "### Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Sb1XVO8jML14"
      },
      "source": [
        "# https://analyticsindiamag.com/implementing-alexnet-using-pytorch-as-a-transfer-learning-model-in-multi-class-classification/\n",
        "# https://towardsdatascience.com/transfer-learning-in-speech-emotion-recognition-d55b6616ba83\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import csv\n",
        "import itertools\n",
        "from torchmetrics.functional import accuracy, f1_score, recall, precision, confusion_matrix, cohen_kappa\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Establezca nuestra semilla y otras configuraciones para la reproducibilidad\n",
        "np.random.seed(42)                        # Establece la semilla para generar números aleatorio numpy\n",
        "torch.manual_seed(42)                     # Establece la semilla para generar números aleatorio pytorch\n",
        "torch.cuda.manual_seed(42)                # Establece la semilla para generar números aleatorio pytorch cuda\n",
        "torch.backends.cudnn.benchmark = False    # True, hace que cuDNN compare múltiples algoritmos de convolución y seleccione el más rápido\n",
        "torch.backends.cudnn.deterministic = True # True, hace que cuDNN solo use algoritmos de convolución deterministas\n",
        "\n",
        "# Transformaciones para la imagen\n",
        "data_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# CreaDataLoaders\n",
        "def creaDataLoaders(sNombreCarpeta:str, iBatchSize:int, iWorkers:int, lClases):\n",
        "  # Leemos todos los datos\n",
        "  all_dataset = datasets.ImageFolder(root=sNombreCarpeta, transform=data_transform, )\n",
        "\n",
        "  # Cantidades de train y test 80/20\n",
        "  iTrain=int(len(all_dataset)*0.8)\n",
        "  iTest=len(all_dataset)-iTrain\n",
        "\n",
        "  # Dividiendo dataset train y test con elementos aleatorios\n",
        "  train_dataset, test_dataset = torch.utils.data.random_split(all_dataset, [iTrain, iTest], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "  #test_dataset = datasets.ImageFolder(root='spectrograms/'+ESTACION+'/'+CANAL+'/test', transform=data_transform)\n",
        "  trainloader = DataLoader(train_dataset, batch_size=iBatchSize, shuffle=True,  num_workers=iWorkers)\n",
        "  testloader  = DataLoader(test_dataset,  batch_size=iBatchSize, shuffle=False, num_workers=iWorkers)\n",
        "\n",
        "  # (OPCIONAL)Creando un dataloader total de todos los datos, para algun caso, generalmente TEST\n",
        "  all_dataloader  = DataLoader(all_dataset, batch_size=iBatchSize, shuffle=False, num_workers=iWorkers)\n",
        "\n",
        "  # Estadisticas\n",
        "  print('Clases: {} Datos de Entrenamiento:{} Prueba:{} Total:{}'.format(lClases, len(train_dataset), len(test_dataset),len(all_dataset)))\n",
        "\n",
        "  # Retornando Datasets, Loaders y Longitudes\n",
        "  return all_dataloader, trainloader, testloader, len(all_dataset), len(train_dataset), len(test_dataset)\n",
        "\n",
        "# Modelo\n",
        "def creaModelo(sModeloTL:str):\n",
        "  #Modelos pre-entrenados de PyTorch ===========================================\n",
        "  if sModeloTL==TL.AlexNet:\n",
        "    #tModelo=torchvision.models.alexnet(pretrained=True)\n",
        "    tModelo = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)     # https://pytorch.org/hub/pytorch_vision_alexnet\n",
        "  elif sModeloTL==TL.GoogLeNet:\n",
        "    #tModelo = torchvision.models.googlenet(pretrained=True)\n",
        "    tModelo = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)   # https://pytorch.org/hub/pytorch_vision_googlenet\n",
        "  elif sModeloTL==TL.VGG:\n",
        "    #tModelo = torchvision.models.vgg19_bn(pretrained=True)\n",
        "    tModelo = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)    # https://pytorch.org/hub/pytorch_vision_vgg\n",
        "  elif sModeloTL==TL.ResNet:\n",
        "    #tModelo = torchvision.models.resnet152(pretrained=True)\n",
        "    tModelo = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)   # https://pytorch.org/hub/pytorch_vision_resnet\n",
        "  else:\n",
        "    print(\"Error modelo indefinido\")\n",
        "\n",
        "  #Descripción modelo\n",
        "  #tModelo.eval()\n",
        "\n",
        "  #Actualizando configuración de modelo ========================================\n",
        "  #Actualizando el segundo clasifidor de tal forma que se tenga 5 clases de salida\n",
        "  if sModeloTL==TL.AlexNet:\n",
        "    tModelo.classifier[4] = nn.Linear(4096, 1024)               #Actualizando el segundo clasificador\n",
        "    tModelo.classifier[6] = nn.Linear(1024, FLAGS.num_classes)  #Actualización del tercer(último) clasificador\n",
        "  elif sModeloTL==TL.GoogLeNet:\n",
        "    tModelo.fc = nn.Linear(1024, FLAGS.num_classes)             #Actualizando el último clasificador\n",
        "  elif sModeloTL==TL.VGG:\n",
        "    tModelo.classifier[3] = nn.Linear(4096, 1024)               #Actualizando el segundo clasificador\n",
        "    tModelo.classifier[6] = nn.Linear(1024, FLAGS.num_classes)  #Actualización del tercer(último) clasificador\n",
        "  elif sModeloTL==TL.ResNet:\n",
        "    tModelo.fc = nn.Linear(2048, FLAGS.num_classes)             #Actualizando el último clasificador\n",
        "  else:\n",
        "    print(\"Error modelo indefinido\")\n",
        "\n",
        "  #Descripción modelo\n",
        "  #TL_Modelo.eval()\n",
        "  return tModelo\n",
        "\n",
        "# Entrenamiento y prueba =======================================================\n",
        "def train_model(model, loss_function, optimizer, data_loader):\n",
        "  # Modelo en modo entrenamiento\n",
        "  model.train()\n",
        "  current_loss = 0.0\n",
        "  current_acc = 0\n",
        "  # Iterar sobre los datos\n",
        "  for i, (inputs, labels) in enumerate(data_loader):\n",
        "    # Enviar input/labels a la GPU\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    # Parámetros gradiente a cero\n",
        "    optimizer.zero_grad()\n",
        "    with torch.set_grad_enabled(True):\n",
        "      # Forward (Predecir clases a partir de imágenes)\n",
        "      outputs = model(inputs)\n",
        "      _, predictions = torch.max(outputs, 1)\n",
        "      # Calcule la pérdida según las predicciones y las etiquetas reales\n",
        "      loss = loss_function(outputs, labels)\n",
        "      # Backward (Retropropagar la pérdida)\n",
        "      loss.backward()\n",
        "      # Ajustar parámetros de acuerdo a los gradientes calculados\n",
        "      optimizer.step()\n",
        "    # Estadísticas\n",
        "    current_loss += loss.item() * inputs.size(0)\n",
        "    current_acc += torch.sum(predictions == labels.data)\n",
        "  total_loss = current_loss / len(data_loader.dataset)\n",
        "  total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "  return total_loss, total_acc.cpu()\n",
        "\n",
        "def test_model(model, loss_function, data_loader):\n",
        "  # Modelo en modo evaluación\n",
        "  model.eval()\n",
        "  current_loss = 0.0\n",
        "  current_acc = 0\n",
        "  # Iterar sobre los datos\n",
        "  for i, (inputs, labels) in enumerate(data_loader):\n",
        "    # Enviar input/labels a la GPU\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    # Forward (Predecir clases a partir de imágenes)\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      _, predictions = torch.max(outputs, 1)\n",
        "      loss = loss_function(outputs, labels)\n",
        "    # Estadísticas\n",
        "    current_loss += loss.item() * inputs.size(0)\n",
        "    current_acc += torch.sum(predictions == labels.data)\n",
        "  total_loss = current_loss / len(data_loader.dataset)\n",
        "  total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "  return total_loss, total_acc.cpu()\n",
        "\n",
        "# Guarda en archivo gráficos de Loss y Acurracy de Entrenamiento y Test ========\n",
        "def plotLossAcurracy(sNombreArchivo:str, vEpocas, vLossTrain, vLossTest, vAccurracyTrain, vAccuracyTest):\n",
        "  fig = plt.figure(figsize=(12, 4))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(vEpocas, vLossTrain, 'g', label='Training loss')\n",
        "  plt.plot(vEpocas, vLossTest, 'b', label='Test loss')\n",
        "  plt.title('Training and Test loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(vEpocas, vAccurracyTrain, 'g', label='Training accuracy')\n",
        "  plt.plot(vEpocas, vAccuracyTest, 'b', label='Test accuracy')\n",
        "  plt.title('Training and Test accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  #plt.savefig(sNombreArchivo, dpi=100, aspect='normal', bbox_inches='tight', pad_inches=0)\n",
        "  plt.savefig(sNombreArchivo, dpi=100, bbox_inches='tight', pad_inches=0)\n",
        "  plt.clf()\n",
        "  plt.close('all')\n",
        "\n",
        "# Guarda CVS con metricas y retorna matriz de confusión ========================\n",
        "def csvMetricas(sNombreArchivo:str, tModelo, tDataLoader, lClases, iEpocas, iDatasetTotal, iDatasetTrain, iDatasetTest):\n",
        "  # Construyendo arreglo de predicciones y datos reales de DATASET TEST\n",
        "  vPrediccion = torch.tensor([],dtype=torch.int64).cuda()\n",
        "  vReal       = torch.tensor([],dtype=torch.int64).cuda()\n",
        "  with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(tDataLoader):\n",
        "      inputs,classes = inputs.to(device), classes.to(device)\n",
        "      outputs = tModelo(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      #print('Clase:{} Predicción:{}'.format(classes, preds))\n",
        "      vPrediccion = torch.cat((vPrediccion, preds), dim=0)\n",
        "      vReal       = torch.cat((vReal, classes), dim=0)\n",
        "\n",
        "  # Generando y guardando en métricas en CSV\n",
        "  confusionmatrix=confusion_matrix(vPrediccion, vReal, num_classes=len(lClases), task='multiclass').cpu()\n",
        "  lAccuracyClass=[]\n",
        "  for i in range(len(lClases)):\n",
        "    lAccuracyClass.append(100*(confusionmatrix.diag()/confusionmatrix.sum(1))[i].cpu().numpy())\n",
        "  average_accuracy=((100.0*confusionmatrix.diag()/confusionmatrix.sum(1)).sum())/len(lClases)\n",
        "  # Creando CSV\n",
        "  with open(sNombreArchivo, mode='w') as csv_file:\n",
        "    csv_wrt = csv.writer(csv_file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    csv_wrt.writerow(['Epochs','Classes','Total','Train','Test','Time','Accuracy','F1','Recall','Precision','Cohen Kappa','Average Acurracy','Class Accuracy'])\n",
        "    csv_wrt.writerow([iEpocas,'-'.join(lClases), iDatasetTotal, iDatasetTrain, iDatasetTest, tTiempoEntrenamiento,\n",
        "                      100*accuracy(vPrediccion,     vReal, num_classes=len(lClases), average='macro', task='multiclass').cpu().numpy(),\n",
        "                      100*f1_score(vPrediccion,     vReal, num_classes=len(lClases), average='macro', task='multiclass').cpu().numpy(),\n",
        "                      100*recall(vPrediccion,       vReal, num_classes=len(lClases), average='macro', task='multiclass').cpu().numpy(),\n",
        "                      100*precision(vPrediccion,    vReal, num_classes=len(lClases), average='macro', task='multiclass').cpu().numpy(),\n",
        "                      100*cohen_kappa(vPrediccion,  vReal, num_classes=len(lClases), task='multiclass').cpu().numpy(),\n",
        "                      average_accuracy.cpu().numpy()\n",
        "                    ] + lAccuracyClass\n",
        "                    )\n",
        "  # Retorna matriz de confusion calculada\n",
        "  return confusionmatrix\n",
        "\n",
        "# Graficar matriz de consusión =================================================\n",
        "# Calcula matriz de confusión\n",
        "def plotConfusionMatrixSimple(vConfusionMatrix, lClases, bNormalized:bool=False, sTitle:str='Confusion matrix', cmap=plt.cm.Blues):\n",
        "  if bNormalized:\n",
        "    vConfusionMatrix = vConfusionMatrix/vConfusionMatrix.sum(axis=1)[:, np.newaxis]\n",
        "    #print(\"Matriz de confusión Normalizeda\")\n",
        "  #else:\n",
        "    #print('Matriz de Confusión, sin normalización')\n",
        "\n",
        "  #print(vConfusionMatrix)\n",
        "  plt.imshow(vConfusionMatrix, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(sTitle)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(lClases))\n",
        "  plt.xticks(tick_marks, lClases, rotation=0)\n",
        "  plt.yticks(tick_marks, lClases)\n",
        "\n",
        "  fmt = '.2f' if bNormalized else '.0f'\n",
        "  thresh = vConfusionMatrix.max()/2.\n",
        "  for i, j in itertools.product(range(vConfusionMatrix.shape[0]), range(vConfusionMatrix.shape[1])):\n",
        "    plt.text(j, i, format(vConfusionMatrix[i,j].item(), fmt), horizontalalignment=\"center\", color=\"white\" if vConfusionMatrix[i, j]>thresh else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "# Grafica matriz de confusión sin normalización y normalizada\n",
        "def plotConfusionMatrix(sNombreArchivo:str, vConfusionMatrix, lClases):\n",
        "  fig = plt.figure(figsize=(10, 5))\n",
        "  #plt.suptitle('Confusion matrix - Station:'+ESTACION+' Channel:'+CANAL)\n",
        "  plt.subplot(1,2,1)\n",
        "  plotConfusionMatrixSimple(vConfusionMatrix, lClases, sTitle='Confusion matrix, without normalization')\n",
        "  plt.subplot(1,2,2)\n",
        "  plotConfusionMatrixSimple(vConfusionMatrix, lClases, bNormalized=True, sTitle='Normalized confusion matrix')\n",
        "  fig.tight_layout(pad=2.0)\n",
        "  #plt.savefig(sNombreArchivo, dpi=100, aspect='normal', bbox_inches='tight', pad_inches=0)\n",
        "  plt.savefig(sNombreArchivo, dpi=100, bbox_inches='tight', pad_inches=0)\n",
        "  plt.clf()\n",
        "  plt.close('all')\n",
        "\n",
        "\n",
        "# Guardar y recuperar modelos de disco =========================================\n",
        "# Guardar modelo y atributos\n",
        "def modelo_guardar(sNombreArchivo:str, tModelo, tFlag, vLossTrain, vLossTest, vAccuracyTrain, vAccuracyTest, tTimeTrain):\n",
        "    torch.save({\n",
        "                'carpeta'       :tFlag.carpeta,\n",
        "                'clases'        :tFlag.classes,\n",
        "                'num_classes'   :tFlag.num_classes,\n",
        "                'batch_size'    :tFlag.batch_size,\n",
        "                'num_workers'   :tFlag.num_workers,\n",
        "                'learning_rate' :tFlag.learning_rate,\n",
        "                'num_epochs'    :len(vLossTrain),         # tFlag.num_epochs,\n",
        "                'time_train'    :tTimeTrain,              # Tiempo de Entrenamiento\n",
        "                'modelo'        :tModelo.state_dict(),\n",
        "                #'optimizador'  :optimizer.state_dict(),\n",
        "                'train_loss'    :vLossTrain,\n",
        "                'test_loss'     :vLossTest,\n",
        "                'train_acc'     :vAccuracyTrain,\n",
        "                'test_acc'      :vAccuracyTest,\n",
        "                }, sNombreArchivo)\n",
        "\n",
        "# Carga modelo desde archivo\n",
        "def modelo_carga(sNombreArchivo:str, tModelo, tFlag):\n",
        "  # Recuperando valores(modelos) de disco\n",
        "  tArchivo  = torch.load(sNombreArchivo)\n",
        "\n",
        "  # Recuperando parametros\n",
        "  tFlag.carpeta = tArchivo['carpeta']\n",
        "  tFlag.classes = tArchivo['clases']\n",
        "  tFlag.num_classes = tArchivo['num_classes']\n",
        "  tFlag.batch_size  = tArchivo['batch_size']\n",
        "  tFlag.num_workers = tArchivo['num_workers']\n",
        "  tFlag.learning_rate = tArchivo['learning_rate']\n",
        "  tFlag.num_epochs  = tArchivo['num_epochs']\n",
        "  tTimeTrain        = tArchivo['time_train'] # Tiempo de Entrenamiento\n",
        "  #fTiempProceso ???\n",
        "  vLossTrain        = tArchivo['train_loss']\n",
        "  vLossTest         = tArchivo['test_loss']\n",
        "  vAccuracyTrain    = tArchivo['train_acc']\n",
        "  vAccuracyTest     = tArchivo['test_acc']\n",
        "\n",
        "  # Recuperando modelo\n",
        "  tModelo.load_state_dict(tArchivo['modelo'])\n",
        "\n",
        "  # Retornando\n",
        "  return tModelo, tFlag, vLossTrain, vLossTest, vAccuracyTrain, vAccuracyTest, tTimeTrain\n",
        "\n",
        "\n",
        "# Métodos para PRUEBA ==========================================================\n",
        "# Entrenamiento con solo prueba del modelo\n",
        "def entrenamiento_solo_prueba(tModelo, lossFunction, testLoader, tTiempoEntrenado:int=0):\n",
        "  # Entrenamiento de modelo que toma eventos reconstruidos\n",
        "  tTiempoUsado=time.time()\n",
        "\n",
        "  fTestLoss, fTestAcc   = test_model(tModelo, lossFunction, testLoader)\n",
        "  print('Test Loss: {:.4f}; Accuracy: {:.4f};  Time:{}  Tiempo Usado:{}s'.format(fTestLoss, fTestAcc, time.asctime(), round(time.time()-tTiempoUsado,1) ))\n",
        "\n",
        "  tTiempoUsado=(time.time() - tTiempoUsado)/60.0\n",
        "  # Actualizando tiempo\n",
        "  tTiempoUsado+=tTiempoEntrenado\n",
        "  print('Finalizó Prueba de datos nuevos sobre Modelo')\n",
        "  # Retorno\n",
        "  return tTiempoUsado, fTestLoss, fTestAcc\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU7FF2cY_k6h"
      },
      "source": [
        "## Proceso varios datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "qqGbiz2TML2S"
      },
      "source": [
        "for i in range(FLAGS.iPorcentajeTiempoInicio, FLAGS.iPorcentajeTiempoFinal+1, FLAGS.iPorcentajeTiempoInc): # range(FLAGS.iPorcentajeTiempoInicio, FLAGS.iPorcentajeTiempoFinal+1):\n",
        "  print(\"Para dataset: \", i, '-', i+FLAGS.iPorcentajeTiempoLong, time.asctime(), '='*70)\n",
        "\n",
        "  # Abrir dados ================================================================\n",
        "  #Elimina folder de espectrogramas si existe\n",
        "  !if [ -d \"spectrograms224_00\" ]; then rm -rf spectrograms224_00; fi\n",
        "\n",
        "  # Descomprimir espectrogramas\n",
        "  !unzip {'\"/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3/spectrograms224_00.'+str(i).zfill(2)+'-'+str(i+FLAGS.iPorcentajeTiempoLong).zfill(2)+'.zip\"'} &> /dev/null\n",
        "\n",
        "  # Crear los dataloaders\n",
        "  _, trainloader, testloader, iTotal, iTrain, iTest = creaDataLoaders(FLAGS.carpeta, FLAGS.batch_size, FLAGS.num_workers, FLAGS.classes)\n",
        "\n",
        "  # Entrenar con modelo TL =====================================================\n",
        "  for sTL in FLAGS.tls:\n",
        "    print('Inicio Entrenamiento de Modelo:'+sTL, time.asctime())\n",
        "\n",
        "    # Crea modelo\n",
        "    TL_Modelo = creaModelo(sTL)\n",
        "\n",
        "    #GPU =======================================================================\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Mover la entrada y modelo TL a GPU para velocidad si esta disponible\n",
        "    TL_Modelo.to(device)\n",
        "\n",
        "    #Funcion de pérdida y optimizador ==========================================\n",
        "    #Función de Pérdida: Entropía cruzada\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    #Optimizador: Algoritmo de Adam\n",
        "    optimizer = optim.Adam(TL_Modelo.parameters(), lr=FLAGS.learning_rate)\n",
        "\n",
        "    # Entrenamiento y prueba ===================================================\n",
        "    tTiempoEntrenamiento=time.time()\n",
        "    vTrainLoss, vTestLoss, vTrainAcc, vTestAcc=[], [], [], []\n",
        "    for epoch in range(FLAGS.num_epochs):\n",
        "      train_loss, train_acc = train_model(TL_Modelo, loss_function, optimizer, trainloader)\n",
        "      test_loss, test_acc = test_model(TL_Modelo, loss_function, testloader)\n",
        "      vTrainLoss.append(train_loss)\n",
        "      vTestLoss.append(test_loss)\n",
        "      vTrainAcc.append(train_acc)\n",
        "      vTestAcc.append(test_acc)\n",
        "      if (epoch+1)%5==0:\n",
        "        print('Época {}/{} Train Loss: {:.4f}; Accuracy: {:.4f} Test Loss: {:.4f}; Accuracy: {:.4f};  Time:{}'.format(epoch + 1, FLAGS.num_epochs, train_loss, train_acc, test_loss, test_acc, time.asctime()))\n",
        "    tTiempoEntrenamiento=(time.time() - tTiempoEntrenamiento)/60.0\n",
        "    print('Finalizó Entrenamiento de Modelo:'+sTL, time.asctime())\n",
        "\n",
        "    # Graficos de Loss y Accuracy ==============================================\n",
        "    plotLossAcurracy(sTL+'_LossAcc_'+str(i).zfill(2)+'-'+str(i+FLAGS.iPorcentajeTiempoLong).zfill(2)+'.png', range(1, FLAGS.num_epochs+1), vTrainLoss, vTestLoss, vTrainAcc, vTestAcc)\n",
        "\n",
        "    # Guardando CVS de métricas y calculando matriz de confusión ===============\n",
        "    vMatrizConfusion = csvMetricas(sTL+'_Metricas_'+str(i).zfill(2)+'-'+str(i+FLAGS.iPorcentajeTiempoLong).zfill(2)+'.csv', TL_Modelo, testloader, FLAGS.classes, FLAGS.num_epochs, iTotal, iTrain, iTest)\n",
        "\n",
        "    # Gráfico de matriz de confusión\n",
        "    plotConfusionMatrix(sTL+'_MatConf_'+str(i).zfill(2)+'-'+str(i+FLAGS.iPorcentajeTiempoLong).zfill(2)+'.png', vMatrizConfusion, FLAGS.classes)\n",
        "\n",
        "    # Mover resultados a Google Drive\n",
        "    !mv {sTL+'_LossAcc_'+str(i).zfill(2)+'-'+str(i+FLAGS.iPorcentajeTiempoLong).zfill(2)+'.png'}  {'\"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3 metricas/\"'}\n",
        "    !mv {sTL+'_Metricas_'+str(i).zfill(2)+'-'+str(i+FLAGS.iPorcentajeTiempoLong).zfill(2)+'.csv'} {'\"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3 metricas/\"'}\n",
        "    !mv {sTL+'_MatConf_'+str(i).zfill(2)+'-'+str(i+FLAGS.iPorcentajeTiempoLong).zfill(2)+'.png'}  {'\"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3 metricas/\"'}\n",
        "\n",
        "  # Eliminar carpeta de espectrogramas\n",
        "  !rm -rf spectrograms224_00\n",
        "\n",
        "  # Destruyendo variables\n",
        "  del TL_Modelo\n",
        "  del trainloader\n",
        "  del testloader\n",
        "print(\"FIN DE PROCESO =========\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su8F9vNPWIO5"
      },
      "source": [
        "## (Opcional) Proceso simple dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XOv1nU4WjKO"
      },
      "source": [
        "### Descomprimir simple dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-u_7hihWogf"
      },
      "source": [
        "# Descomprimir dataset\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 221012 undersampling/spectrograms224_00 221012 10Hz.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion4/spectrograms224_00.05-25_20Hz.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion4/spectrograms224_00.05-25_20Hz.zip 67095' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3/spectrograms224_00.05-25_20Hz.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz+da_jittering 0.01-0.1.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz+da_jittering 0.001-0.01.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz+da_jittering 0.0001-0.001.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz+da_jittering 0.01-0.1 67095.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz+da_jittering 0.001-0.01 67095.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz+da_jittering 0.0001-0.001 67095.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/01/spectrograms224_00 231211 20Hz+da_sa 0.15-2 0.15-2 sin corte ruido.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/01/spectrograms224_00 231218 20Hz+da_interpolation 40-60.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/01/spectrograms224_00 210708 20Hz+da_jittering 0.2 real+artificial.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/01/spectrograms224_00 210708 20Hz+da_jittering 0.2 artificial.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231211 20Hz+da_ag 5_3.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231211 20Hz+da_ag 5_5.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231211 20Hz+da_ag1 0.4_10T.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231211 20Hz+da_ag1 0.45_10T.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231211 20Hz+da_sa 0.1-2 0.1-2.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231211 20Hz+da_sa 0.15-2 0.15-2.zip' &> /dev/null\n",
        "!unzip '/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231218 20Hz+da_interpolation 40-60.zip' &> /dev/null"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extrae muestra del dataset actual de archivos (Opcional)"
      ],
      "metadata": {
        "id": "0gIjmRUdupCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, shutil\n",
        "from random import randint, choice, sample\n",
        "\n",
        "def generaMuestraArchivos(sRutaEntrada:str, sRutaSalida:str, dEvento:dict):\n",
        "  \"\"\"Genera una muestra aleatoria de los eventos sin repeticion en carpeta destino.\n",
        "  Args:\n",
        "      sRutaEntrada (str): Carpeta donde se encuentan los eventos originales organizado por carpetas Evento\n",
        "      sRutaSalida (str):  Carpeta donde se generaran la muestra de eventos\n",
        "      dEvento (dict): Diccionario de eventos y cantidades a generer por evento, Ej. {'HY':2000, 'LP':1500, 'TC':0, 'TR':1800, 'VT':1350}\n",
        "  \"\"\"\n",
        "  # Leyendo eventos\n",
        "  for sEvento in dEvento:\n",
        "    print(\"Generando muestra de eventos:\", sEvento)\n",
        "    # Leyendo lista de archivo de eventos desde carpeta\n",
        "    #m=fym.lista_archivos_simple(sRutaEntrada+sEvento)\n",
        "    m=list(map(os.path.basename, glob.glob(sRutaEntrada+sEvento+\"/*.*\")))\n",
        "    if len(m)>0:\n",
        "      # Generando lista que tiene muestra de eventos sin repeticion\n",
        "      lMuestra = sample(m, dEvento[sEvento])\n",
        "      # Crear carpetas de salida si no existen\n",
        "      if not os.path.exists(sRutaSalida+sEvento):\n",
        "        os.makedirs(sRutaSalida+sEvento)\n",
        "      # Copiar archivos a ruta de salida\n",
        "      for fArchivo in lMuestra:\n",
        "        shutil.copy(sRutaEntrada+sEvento+'/'+fArchivo, sRutaSalida+sEvento)\n",
        "    # Mensaje\n",
        "    print(\"Fin de generación de eventos :\", sEvento)\n",
        "  return\n",
        "\n",
        "# Crear carpeta temporal\n",
        "!mkdir spectrograms224\n",
        "# Muestra a extraer\n",
        "generaMuestraArchivos('spectrograms224_00/', 'spectrograms224/', {'HY':2686, 'LP':2686, 'TC':2686 ,'TR':2686, 'VT':2686})\n",
        "#Eliminar carpeta original\n",
        "!rm -rf spectrograms224_00\n",
        "# Renombra carpeta\n",
        "!mv spectrograms224 spectrograms224_00"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "009ZmrJtTDFg",
        "outputId": "e8004bb0-4e0f-4cd7-fcbb-07df1298d169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando muestra de eventos: HY\n",
            "Fin de generación de eventos : HY\n",
            "Generando muestra de eventos: LP\n",
            "Fin de generación de eventos : LP\n",
            "Generando muestra de eventos: TC\n",
            "Fin de generación de eventos : TC\n",
            "Generando muestra de eventos: TR\n",
            "Fin de generación de eventos : TR\n",
            "Generando muestra de eventos: VT\n",
            "Fin de generación de eventos : VT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y4fbk6hWqtQ"
      },
      "source": [
        "### Entrenar modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4pqXpAcWPVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266d4eb7-0b9e-4de3-e209-aaa830860583"
      },
      "source": [
        "RUTA_SALIDA=r'/content/gdrive/My Drive/Espectrogramas/Escenario2/01/spectrograms224_00 231218 interpolation/'\n",
        "\n",
        "print(\"Para dataset: \", time.asctime(), '='*70)\n",
        "\n",
        "# Crear los dataloaders\n",
        "_, trainloader, testloader, iTotal, iTrain, iTest = creaDataLoaders(FLAGS.carpeta, FLAGS.batch_size, FLAGS.num_workers, FLAGS.classes)\n",
        "\n",
        "# Entrenar con modelo TL =====================================================\n",
        "for sTL in FLAGS.tls:\n",
        "  print('Inicio Entrenamiento de Modelo:'+sTL, time.asctime())\n",
        "\n",
        "  # Crea modelo\n",
        "  TL_Modelo = creaModelo(sTL)\n",
        "\n",
        "  #GPU =======================================================================\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #Mover la entrada y modelo TL a GPU para velocidad si esta disponible\n",
        "  TL_Modelo.to(device)\n",
        "\n",
        "  #Funcion de pérdida y optimizador ==========================================\n",
        "  #Función de Pérdida: Entropía cruzada\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  #Optimizador: Algoritmo de Adam\n",
        "  optimizer = optim.Adam(TL_Modelo.parameters(), lr=FLAGS.learning_rate)\n",
        "\n",
        "  # Entrenamiento y prueba ===================================================\n",
        "  tTiempoEntrenamiento=time.time()\n",
        "  vTrainLoss, vTestLoss, vTrainAcc, vTestAcc=[], [], [], []\n",
        "  for epoch in range(FLAGS.num_epochs):\n",
        "    train_loss, train_acc = train_model(TL_Modelo, loss_function, optimizer, trainloader)\n",
        "    test_loss, test_acc = test_model(TL_Modelo, loss_function, testloader)\n",
        "    vTrainLoss.append(train_loss)\n",
        "    vTestLoss.append(test_loss)\n",
        "    vTrainAcc.append(train_acc)\n",
        "    vTestAcc.append(test_acc)\n",
        "    if (epoch+1)%5==0:\n",
        "      print('Época {}/{} Train Loss: {:.4f}; Accuracy: {:.4f} Test Loss: {:.4f}; Accuracy: {:.4f};  Time:{}'.format(epoch + 1, FLAGS.num_epochs, train_loss, train_acc, test_loss, test_acc, time.asctime()))\n",
        "  tTiempoEntrenamiento=(time.time() - tTiempoEntrenamiento)/60.0\n",
        "  print('Finalizó Entrenamiento de Modelo:'+sTL, time.asctime())\n",
        "\n",
        "  # Graficos de Loss y Accuracy ==============================================\n",
        "  plotLossAcurracy(RUTA_SALIDA+sTL+'_LossAcc.png', range(1, FLAGS.num_epochs+1), vTrainLoss, vTestLoss, vTrainAcc, vTestAcc)\n",
        "\n",
        "  # Guardando CVS de métricas y calculando matriz de confusión ===============\n",
        "  vMatrizConfusion = csvMetricas(RUTA_SALIDA+sTL+'_Metricas.csv', TL_Modelo, testloader, FLAGS.classes, FLAGS.num_epochs, iTotal, iTrain, iTest)\n",
        "\n",
        "  # Gráfico de matriz de confusión\n",
        "  plotConfusionMatrix(RUTA_SALIDA+sTL+'_MatConf.png', vMatrizConfusion, FLAGS.classes)\n",
        "\n",
        "  # Guardar Modelo\n",
        "  modelo_guardar(RUTA_SALIDA+sTL+'_Modelo.pt', TL_Modelo,  FLAGS, vTrainLoss, vTestLoss, vTrainAcc, vTestAcc, tTiempoEntrenamiento)\n",
        "\n",
        "  # Mover resultados a Google Drive\n",
        "  #!mv {sTL+'_LossAcc.png'}  {'\"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3/\"'}\n",
        "  #!mv {sTL+'_Metricas.csv'} {'\"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3/\"'}\n",
        "  #!mv {sTL+'_MatConf.png'}  {'\"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3/\"'}\n",
        "  #!mv {sTL+'_Modelo.pt'}    {'\"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3/\"'}\n",
        "\n",
        "print(\"FIN DE PROCESO =========\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para dataset:  Mon Jun 16 20:39:16 2025 ======================================================================\n",
            "Clases: ['HY', 'LP', 'TC', 'TR', 'VT'] Datos de Entrenamiento:10744 Prueba:2686 Total:13430\n",
            "Inicio Entrenamiento de Modelo:AlexNet Mon Jun 16 20:39:16 2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 5/20 Train Loss: 0.1891; Accuracy: 0.9271 Test Loss: 0.3398; Accuracy: 0.8820;  Time:Mon Jun 16 20:43:25 2025\n",
            "Época 10/20 Train Loss: 0.0747; Accuracy: 0.9747 Test Loss: 0.4985; Accuracy: 0.8861;  Time:Mon Jun 16 20:47:34 2025\n",
            "Época 15/20 Train Loss: 0.0500; Accuracy: 0.9814 Test Loss: 0.5064; Accuracy: 0.8831;  Time:Mon Jun 16 20:51:42 2025\n",
            "Época 20/20 Train Loss: 0.0390; Accuracy: 0.9873 Test Loss: 0.7971; Accuracy: 0.8727;  Time:Mon Jun 16 20:55:51 2025\n",
            "Finalizó Entrenamiento de Modelo:AlexNet Mon Jun 16 20:55:51 2025\n",
            "FIN DE PROCESO =========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T-n6vbqvkj_"
      },
      "source": [
        "# GUARDAR MODELO\n",
        "#modelo_guardar(RUTA_EXPERIMENTO+MODELO_AE+'ModeloREC.pt', modelNN,  FLAGS_NN, vTrainLoss, vTestLoss, vTrainAcc, vTestAcc, tTiempoEntrenamiento)\n",
        "#modelo_guardar(RUTA_EXPERIMENTO+MODELO_AE+'ModeloCOD.pt', modelRN,  FLAGS_CC, vTrainLoss, vTestLoss, vTrainAcc, vTestAcc, tTiempoEntrenamiento)\n",
        "\n",
        "# RECUPERAR MODELO\n",
        "TL_Modelo, FLAGS, vTrainLoss, vTestLoss, vTrainAcc, vTestAcc, tTiempoEntrenamiento = modelo_carga(\n",
        "  \"/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 +daRotacion3 metricas/AlexNet_Modelo.pt\", TL_Modelo, FLAGS)\n",
        "TL_Modelo.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftRWdicecslE"
      },
      "source": [
        "##(Opcional) Prueba del modelo con datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or47PyxOTLvn"
      },
      "source": [
        "### Descomprimir simple dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Elimina folder si es necesario\n",
        "!rm -rf spectrograms224_00"
      ],
      "metadata": {
        "id": "Q8mA8-1kGPCS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sysbGzg_TE2Z"
      },
      "source": [
        "# Descomprimir dataset\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708.zip' &> /dev/null\n",
        "!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/00/spectrograms224_00 210708 20Hz.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/01/spectrograms224_00 231127 20Hz+da_ag1 0.4_10T con corte ruido.zip' &> /dev/null\n",
        "#!unzip '/content/gdrive/My Drive/Espectrogramas/Escenario2/01/spectrograms224_00 231127 20Hz+da_sa 0.1-2 0.1-2 con corte ruido.zip' &> /dev/null"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cargar modelo si no se tiene (OPCIONAL)"
      ],
      "metadata": {
        "id": "oWPr05d153o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes\n",
        "#RUTA_ENTRADA='/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 221012 undersampling/20Hz 20epocas/'\n",
        "#RUTA_SALIDA ='/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 221012 undersampling/'\n",
        "#RUTA_ENTRADA='/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 +daJittering 20Hz/0.0001-0.001 67095/'\n",
        "#RUTA_SALIDA ='/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 +daJittering 20Hz/'\n",
        "#RUTA_ENTRADA='/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 +daJittering 20Hz/0.0001-0.001 13430 Artificial/'\n",
        "#RUTA_SALIDA ='/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 +daJittering 20Hz/'\n",
        "RUTA_ENTRADA='/content/gdrive/MyDrive/Espectrogramas/Escenario2/00/spectrograms224_00 +daJittering 20Hz/0.01-0.1 67095 Artificial/'\n",
        "RUTA_SALIDA ='/content/gdrive/MyDrive/Espectrogramas/Escenario2/01/spectrograms224_00 231127 sa/'"
      ],
      "metadata": {
        "id": "GCng2tXqDPTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGAR MODELO SI NO SE TIENE (OPCIONAL)\n",
        "# Crea modelo en blanco del requerido\n",
        "TL_Modelo = creaModelo(TL.AlexNet)\n",
        "\n",
        "#GPU =======================================================================\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Mover el modelo TL a GPU para velocidad si esta disponible\n",
        "TL_Modelo.to(device)\n",
        "\n",
        "#Función de Pérdida: Entropía cruzada\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# RECUPERAR MODELO SI NO SE TIENE CARGADO(OCIONAL)\n",
        "TL_Modelo, FLAGS, vTrainLoss, vTestLoss, vTrainAcc, vTestAcc, tTiempoEntrenamiento = modelo_carga(RUTA_ENTRADA+'AlexNet_Modelo.pt', TL_Modelo, FLAGS)\n",
        "TL_Modelo.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPoQHSK-wX_Z",
        "outputId": "5bb41f50-39ee-4082-f5ca-aad8eb1d2010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=1024, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Probar el modelo"
      ],
      "metadata": {
        "id": "jkbb4D-tFOUR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz3e8GnQerH1",
        "outputId": "f9c39f62-08ba-4116-e43a-d83cdfefb2d0"
      },
      "source": [
        "# Crear los dataloaders\n",
        "all_dataloader, trainloader, testloader, iTotal, iTrain, iTest = creaDataLoaders(FLAGS.carpeta, FLAGS.batch_size, FLAGS.num_workers, FLAGS.classes)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases: ['HY', 'LP', 'TC', 'TR', 'VT'] Datos de Entrenamiento:4916 Prueba:1229 Total:6145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCYKi0vYcypd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f108b642-2038-4b2c-a559-a7ca891fc005"
      },
      "source": [
        "# Se debe tener el modelo TL_Modelo cargado\n",
        "\n",
        "# Prueba sobre datos de prueba\n",
        "tTiempoEntrenamiento, vTestLoss, vTestAcc=entrenamiento_solo_prueba(TL_Modelo, loss_function, all_dataloader)\n",
        "\n",
        "# Métricas\n",
        "vMatrizConfusionRecPruebaTL = csvMetricas(RUTA_SALIDA+'Rec Real Metricas.csv', TL_Modelo, all_dataloader, FLAGS.classes, -1, iTotal, iTotal, iTotal)\n",
        "# Gráfico de matriz de confusión\n",
        "plotConfusionMatrix(RUTA_SALIDA+'Rec Real MatConf.png', vMatrizConfusionRecPruebaTL, FLAGS.classes)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2998; Accuracy: 0.9473;  Time:Mon Jun 16 20:57:12 2025  Tiempo Usado:16.0s\n",
            "Finalizó Prueba de datos nuevos sobre Modelo\n"
          ]
        }
      ]
    }
  ]
}